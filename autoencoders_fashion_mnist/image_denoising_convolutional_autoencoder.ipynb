{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_denoising_convolutional_autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74LfkAExb7AF",
        "colab_type": "text"
      },
      "source": [
        "# Image Denoising with Denoising AutoEncoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRQKk1cEbyvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1d1bdee7-f092-4ecc-c1df-8616b635f939"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input, Conv2D, MaxPool2D, UpSampling2D\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gBZLwswcV1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoSkzQnncstq",
        "colab_type": "text"
      },
      "source": [
        "### Data Pre-processing\n",
        "We need to augment our input image in order to deliberately make noises in the images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGzW_T17cmgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using salt and Pepper\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "seq = iaa.Sequential([iaa.SaltAndPepper(0.2)])\n",
        "\n",
        "train_x_aug = seq.augment_images(train_x)\n",
        "val_x_aug = seq.augment_images(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrleFsl7lCRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "f39f7b49-ce0f-4feb-bfcc-2a0098ec7ce8"
      },
      "source": [
        "# normalization\n",
        "\n",
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 28, 28, 1)\n",
        "val_x = val_x.reshape(-1, 28, 28, 1)\n",
        "\n",
        "train_x_aug = train_x_aug/255.\n",
        "val_x_aug = val_x_aug/255.\n",
        "\n",
        "train_x_aug = train_x_aug.reshape(-1, 28, 28, 1)\n",
        "val_x_aug = val_x_aug.reshape(-1, 28, 28, 1)\n",
        "\n",
        "plt.imshow(val_x_aug[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f59ed18ba90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4fyT0-lRkb",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhAelgt9lOaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "encoded = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPool2D((2, 2), padding='same')(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY7XQEG8lawj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "decoded = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "decoded = Conv2D(64, (3, 3), activation='relu')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "decoded = Conv2D(1, (3, 3), padding='same')(decoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rDbvH5flhJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "27977be2-925d-4076-bd88-28779a40ce52"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 16)          4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 32)          4640      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
            "=================================================================\n",
            "Total params: 49,761\n",
            "Trainable params: 49,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlGl7mDalmfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bJ6Qq5ylpaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adam', \n",
        "                    loss='mse'\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF1OostyluLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                      min_delta=0, \n",
        "                                      patience=10, \n",
        "                                      verbose=1, \n",
        "                                      mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en2Vai0xl4rD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61ee99ce-476a-41e4-c2a8-bd2037d57b97"
      },
      "source": [
        "train_history = autoencoder.fit(train_x_aug, \n",
        "                                train_x, \n",
        "                                epochs=500,\n",
        "                                batch_size=2048, \n",
        "                                validation_data=(val_x_aug, val_x), \n",
        "                                callbacks=[estop])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 15:56:52.911242 140026007234432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0819 15:56:53.123468 140026007234432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0946 - val_loss: 0.0512\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0419 - val_loss: 0.0353\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 7s 119us/step - loss: 0.0318 - val_loss: 0.0282\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0263 - val_loss: 0.0255\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0237 - val_loss: 0.0227\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0221 - val_loss: 0.0217\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0214 - val_loss: 0.0208\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0205 - val_loss: 0.0202\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0200 - val_loss: 0.0196\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0194 - val_loss: 0.0192\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0191 - val_loss: 0.0189\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0186 - val_loss: 0.0184\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0183 - val_loss: 0.0180\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0182 - val_loss: 0.0180\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0177 - val_loss: 0.0175\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0173 - val_loss: 0.0174\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0172 - val_loss: 0.0172\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0169 - val_loss: 0.0169\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0166 - val_loss: 0.0167\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0170 - val_loss: 0.0164\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0163 - val_loss: 0.0163\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0162 - val_loss: 0.0163\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0160 - val_loss: 0.0160\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0159 - val_loss: 0.0158\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0160 - val_loss: 0.0174\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0158 - val_loss: 0.0156\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0154 - val_loss: 0.0155\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0154 - val_loss: 0.0153\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0153 - val_loss: 0.0153\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0153 - val_loss: 0.0157\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0151 - val_loss: 0.0150\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0149 - val_loss: 0.0149\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0150 - val_loss: 0.0154\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0148 - val_loss: 0.0147\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0148 - val_loss: 0.0148\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0146 - val_loss: 0.0146\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0146 - val_loss: 0.0150\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0145 - val_loss: 0.0144\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0143 - val_loss: 0.0143\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0144 - val_loss: 0.0143\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0142 - val_loss: 0.0142\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0142 - val_loss: 0.0143\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0141 - val_loss: 0.0142\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0140 - val_loss: 0.0140\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0140 - val_loss: 0.0145\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0138 - val_loss: 0.0138\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0140 - val_loss: 0.0141\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0137 - val_loss: 0.0137\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0137 - val_loss: 0.0137\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0135 - val_loss: 0.0139\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0136 - val_loss: 0.0136\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0136 - val_loss: 0.0135\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0135 - val_loss: 0.0135\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0134 - val_loss: 0.0142\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0135 - val_loss: 0.0134\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0133 - val_loss: 0.0133\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0133 - val_loss: 0.0134\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0132 - val_loss: 0.0140\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0133 - val_loss: 0.0133\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0131 - val_loss: 0.0131\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0132 - val_loss: 0.0131\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0131 - val_loss: 0.0134\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0131 - val_loss: 0.0131\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0129 - val_loss: 0.0130\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0129 - val_loss: 0.0130\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0130 - val_loss: 0.0129\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0129 - val_loss: 0.0131\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0128 - val_loss: 0.0129\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0128 - val_loss: 0.0128\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0128 - val_loss: 0.0128\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0127 - val_loss: 0.0139\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0130 - val_loss: 0.0127\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0126 - val_loss: 0.0126\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0127 - val_loss: 0.0127\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0125 - val_loss: 0.0126\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0126 - val_loss: 0.0127\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0126 - val_loss: 0.0127\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0125 - val_loss: 0.0125\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0125 - val_loss: 0.0129\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0124 - val_loss: 0.0124\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0123 - val_loss: 0.0124\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0127 - val_loss: 0.0126\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0124 - val_loss: 0.0123\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0122 - val_loss: 0.0123\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0122 - val_loss: 0.0123\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0125 - val_loss: 0.0126\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0121 - val_loss: 0.0122\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0122 - val_loss: 0.0123\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0121 - val_loss: 0.0121\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0120 - val_loss: 0.0121\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0122 - val_loss: 0.0121\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0121 - val_loss: 0.0121\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0119 - val_loss: 0.0120\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0119 - val_loss: 0.0121\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0118 - val_loss: 0.0118\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0118 - val_loss: 0.0118\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0117 - val_loss: 0.0118\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0117 - val_loss: 0.0119\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0116 - val_loss: 0.0118\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0117 - val_loss: 0.0116\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0117 - val_loss: 0.0118\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0115 - val_loss: 0.0115\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0116 - val_loss: 0.0116\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0115 - val_loss: 0.0115\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0114 - val_loss: 0.0116\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0116 - val_loss: 0.0115\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0112 - val_loss: 0.0114\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0114 - val_loss: 0.0119\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0113 - val_loss: 0.0115\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0112 - val_loss: 0.0114\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0111 - val_loss: 0.0117\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0112 - val_loss: 0.0112\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0112 - val_loss: 0.0112\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0111 - val_loss: 0.0112\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0110 - val_loss: 0.0111\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0110 - val_loss: 0.0110\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0116\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0110 - val_loss: 0.0110\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0110 - val_loss: 0.0111\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0110 - val_loss: 0.0120\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0110\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0110\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0110 - val_loss: 0.0109\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0109 - val_loss: 0.0113\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0108 - val_loss: 0.0111\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0108 - val_loss: 0.0113\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0109 - val_loss: 0.0108\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0107 - val_loss: 0.0110\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 183/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 184/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 185/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 186/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0106 - val_loss: 0.0110\n",
            "Epoch 187/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 188/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0105 - val_loss: 0.0110\n",
            "Epoch 189/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 190/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 191/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 192/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0106 - val_loss: 0.0106\n",
            "Epoch 193/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 194/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 195/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 196/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0105 - val_loss: 0.0105\n",
            "Epoch 197/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0106 - val_loss: 0.0106\n",
            "Epoch 198/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0105 - val_loss: 0.0105\n",
            "Epoch 199/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 200/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 201/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 202/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 203/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 204/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 205/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0106\n",
            "Epoch 206/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0108\n",
            "Epoch 207/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0107\n",
            "Epoch 208/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0107\n",
            "Epoch 209/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 210/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 211/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 212/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 213/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0104 - val_loss: 0.0106\n",
            "Epoch 214/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 215/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 216/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 217/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 218/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 219/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 220/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 221/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0103 - val_loss: 0.0105\n",
            "Epoch 222/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 223/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0118\n",
            "Epoch 224/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0106 - val_loss: 0.0104\n",
            "Epoch 225/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 226/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 227/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 228/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 229/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 230/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 231/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 232/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0105 - val_loss: 0.0105\n",
            "Epoch 233/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 234/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 235/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 236/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 237/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 238/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 239/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 240/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 241/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 242/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0104\n",
            "Epoch 243/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0102\n",
            "Epoch 244/500\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 245/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 246/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 247/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 248/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 249/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 250/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0100 - val_loss: 0.0105\n",
            "Epoch 251/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 252/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 253/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 254/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 255/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 256/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 257/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 258/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 259/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 260/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 261/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 262/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0102 - val_loss: 0.0101\n",
            "Epoch 263/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 264/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0101\n",
            "Epoch 265/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 266/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 267/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0100 - val_loss: 0.0116\n",
            "Epoch 268/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0103 - val_loss: 0.0101\n",
            "Epoch 269/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 270/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 271/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0102\n",
            "Epoch 272/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 273/500\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 274/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 275/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 276/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 277/500\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0099 - val_loss: 0.0102\n",
            "Epoch 278/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 279/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0099\n",
            "Epoch 280/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0106\n",
            "Epoch 281/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 282/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0100\n",
            "Epoch 283/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0101\n",
            "Epoch 284/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0099\n",
            "Epoch 285/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 286/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0101\n",
            "Epoch 287/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0102\n",
            "Epoch 288/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0099\n",
            "Epoch 289/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0099\n",
            "Epoch 290/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 291/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 292/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0099\n",
            "Epoch 293/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0100\n",
            "Epoch 294/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0118\n",
            "Epoch 295/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0100\n",
            "Epoch 296/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 297/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0099\n",
            "Epoch 298/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 299/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0099\n",
            "Epoch 300/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0101\n",
            "Epoch 301/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 302/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 303/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0099\n",
            "Epoch 304/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0101\n",
            "Epoch 305/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 306/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0099 - val_loss: 0.0103\n",
            "Epoch 307/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 308/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 309/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 310/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 311/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 312/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0099\n",
            "Epoch 313/500\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 314/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0104\n",
            "Epoch 315/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 316/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0100\n",
            "Epoch 317/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 318/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 319/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0099\n",
            "Epoch 320/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0102\n",
            "Epoch 321/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 322/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0098\n",
            "Epoch 323/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 324/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0098\n",
            "Epoch 325/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 326/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 327/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0099\n",
            "Epoch 328/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0100\n",
            "Epoch 329/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0097\n",
            "Epoch 330/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0097\n",
            "Epoch 331/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 332/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 333/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 334/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0101\n",
            "Epoch 335/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0097\n",
            "Epoch 336/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 337/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 338/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0097 - val_loss: 0.0101\n",
            "Epoch 339/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 340/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 341/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0098 - val_loss: 0.0097\n",
            "Epoch 342/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 343/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 344/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 345/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 346/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 347/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0097\n",
            "Epoch 348/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0104\n",
            "Epoch 349/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0097\n",
            "Epoch 350/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 351/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0098\n",
            "Epoch 352/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0099\n",
            "Epoch 353/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 354/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 355/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 356/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 357/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0097 - val_loss: 0.0097\n",
            "Epoch 358/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0097\n",
            "Epoch 359/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0097\n",
            "Epoch 360/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0098\n",
            "Epoch 361/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 362/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0097\n",
            "Epoch 363/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 364/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0098 - val_loss: 0.0096\n",
            "Epoch 365/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 366/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 367/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 368/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 369/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 370/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0100\n",
            "Epoch 371/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 372/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 373/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 374/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0096 - val_loss: 0.0098\n",
            "Epoch 375/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 376/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0095 - val_loss: 0.0097\n",
            "Epoch 377/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0096 - val_loss: 0.0096\n",
            "Epoch 378/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0094 - val_loss: 0.0097\n",
            "Epoch 379/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 380/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 381/500\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0094 - val_loss: 0.0098\n",
            "Epoch 382/500\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0101 - val_loss: 0.0096\n",
            "Epoch 00382: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z62JH_zvmGmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc4YPMudxTDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "ed439f29-73ca-4c84-8cf3-58f4a20168a7"
      },
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), \n",
        "           cmap='gray')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f59ed201128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EneOiKAtxhKn",
        "colab_type": "text"
      },
      "source": [
        "This is what we get after noise removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Faz2j3egx5qj",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2KoTaJ6xZWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6d292a4c-f462-49f1-dccd-bc6faf27e594"
      },
      "source": [
        "plt.imshow(pred[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f59e0307a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}