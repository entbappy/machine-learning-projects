{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_compression_sparse_autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4u7g2ZWxS-",
        "colab_type": "text"
      },
      "source": [
        "# Image Compression using a Sparse AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMwAh7m-Wyfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6de5b4a8-f1f4-469e-8f45-698353240563"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "# from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['time']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Dlf6suW5Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njjqp7bbW_ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 784)\n",
        "val_x = val_x.reshape(-1, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWow0iXJ3e",
        "colab_type": "text"
      },
      "source": [
        "### Autoencoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22X_rhDLPhEd",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24gAYx-VXDlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(784,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(2000, activation='relu')(input_img)\n",
        "encoded = Dense(500, activation='relu', activity_regularizer=regularizers.l1(10e-10))(encoded)\n",
        "encoded = Dense(500, activation='relu', activity_regularizer=regularizers.l1(10e-10))(encoded)\n",
        "encoded = Dense(10, activation='sigmoid',activity_regularizer=regularizers.l1(10e-10))(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmW_RHtcQXa_",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTdgKWSDP85b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded = Dense(500, activation='relu')(encoded)\n",
        "decoded = Dense(500, activation='relu')(decoded)\n",
        "decoded = Dense(2000, activation='relu')(decoded)\n",
        "decoded = Dense(784)(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mxU14ztQf6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWIRq8vwQqYd",
        "colab_type": "text"
      },
      "source": [
        "### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT7bhq2EQkPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "72dd4a03-3455-459e-dbf5-f07aff751582"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2000)              1570000   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               1000500   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5010      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               5500      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 784)               1568784   \n",
            "=================================================================\n",
            "Total params: 5,652,794\n",
            "Trainable params: 5,652,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzlHcJFRQv2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBVyAdRwSduU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d31952a2-1b1f-4064-8cd0-10b612ecf9c5"
      },
      "source": [
        "autoencoder.compile(optimizer='adam', \n",
        "                    loss='mse')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 14:31:12.496134 139755205625728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl80JD9hSfp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45222464-f577-489e-8b92-2ce776d76207"
      },
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                      min_delta=0, \n",
        "                                      patience=10, \n",
        "                                      verbose=1, \n",
        "                                      mode='auto')\n",
        "\n",
        "train_history = autoencoder.fit(train_x, \n",
        "                                train_x, \n",
        "                                epochs=200, \n",
        "                                batch_size=2048, \n",
        "                                validation_data=(val_x, val_x), \n",
        "                                callbacks=[estop])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 14:43:16.161781 139755205625728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0819 14:43:16.407409 139755205625728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0918 - val_loss: 0.0679\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0589 - val_loss: 0.0474\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0426 - val_loss: 0.0386\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0371 - val_loss: 0.0393\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0340 - val_loss: 0.0318\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0302 - val_loss: 0.0289\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0274 - val_loss: 0.0259\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0253 - val_loss: 0.0246\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0240 - val_loss: 0.0236\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0231 - val_loss: 0.0225\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0222 - val_loss: 0.0217\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0216 - val_loss: 0.0214\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0208 - val_loss: 0.0205\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0204 - val_loss: 0.0203\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0199 - val_loss: 0.0198\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0198 - val_loss: 0.0196\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0193 - val_loss: 0.0193\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0191 - val_loss: 0.0212\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0193 - val_loss: 0.0188\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0183 - val_loss: 0.0182\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0180 - val_loss: 0.0179\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0177 - val_loss: 0.0178\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0173 - val_loss: 0.0171\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0172 - val_loss: 0.0176\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0168 - val_loss: 0.0166\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0164 - val_loss: 0.0165\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0161 - val_loss: 0.0161\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0161 - val_loss: 0.0159\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0158 - val_loss: 0.0157\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0156 - val_loss: 0.0157\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0153 - val_loss: 0.0154\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0151 - val_loss: 0.0153\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0151 - val_loss: 0.0155\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0149 - val_loss: 0.0149\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0148 - val_loss: 0.0149\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0146 - val_loss: 0.0149\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0144 - val_loss: 0.0146\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0145 - val_loss: 0.0145\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0142 - val_loss: 0.0145\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0141 - val_loss: 0.0143\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0140 - val_loss: 0.0142\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0141 - val_loss: 0.0147\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0139 - val_loss: 0.0140\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0137 - val_loss: 0.0145\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0139 - val_loss: 0.0139\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0135 - val_loss: 0.0138\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0136 - val_loss: 0.0139\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0134 - val_loss: 0.0137\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0134 - val_loss: 0.0139\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0134 - val_loss: 0.0136\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0133 - val_loss: 0.0136\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0132 - val_loss: 0.0135\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0133 - val_loss: 0.0135\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0131 - val_loss: 0.0134\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0130 - val_loss: 0.0134\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0132 - val_loss: 0.0134\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0128 - val_loss: 0.0134\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0130 - val_loss: 0.0133\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0129 - val_loss: 0.0134\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0128 - val_loss: 0.0132\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0128 - val_loss: 0.0134\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0127 - val_loss: 0.0132\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0128 - val_loss: 0.0133\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0127 - val_loss: 0.0131\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0127 - val_loss: 0.0131\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0125 - val_loss: 0.0131\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0125 - val_loss: 0.0134\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0126 - val_loss: 0.0130\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0125 - val_loss: 0.0129\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0124 - val_loss: 0.0128\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0124 - val_loss: 0.0131\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0123 - val_loss: 0.0128\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0123 - val_loss: 0.0130\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0123 - val_loss: 0.0127\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0123 - val_loss: 0.0129\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0122 - val_loss: 0.0128\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0121 - val_loss: 0.0127\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0121 - val_loss: 0.0127\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0120 - val_loss: 0.0126\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0121 - val_loss: 0.0129\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0120 - val_loss: 0.0127\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0120 - val_loss: 0.0130\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0120 - val_loss: 0.0125\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0119 - val_loss: 0.0125\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0120 - val_loss: 0.0127\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0119 - val_loss: 0.0124\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0118 - val_loss: 0.0125\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0118 - val_loss: 0.0125\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0119 - val_loss: 0.0124\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0117 - val_loss: 0.0128\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0117 - val_loss: 0.0123\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0117 - val_loss: 0.0125\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0116 - val_loss: 0.0124\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0118 - val_loss: 0.0125\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0115 - val_loss: 0.0122\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0115 - val_loss: 0.0123\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0114 - val_loss: 0.0122\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0115 - val_loss: 0.0121\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0114 - val_loss: 0.0121\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0114 - val_loss: 0.0127\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0115 - val_loss: 0.0121\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0113 - val_loss: 0.0122\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0113 - val_loss: 0.0121\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0114 - val_loss: 0.0121\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0112 - val_loss: 0.0121\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0113 - val_loss: 0.0122\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0113 - val_loss: 0.0121\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0111 - val_loss: 0.0121\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0112 - val_loss: 0.0121\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0113 - val_loss: 0.0121\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0112 - val_loss: 0.0121\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0120\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0121\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0112 - val_loss: 0.0122\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0110 - val_loss: 0.0119\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0110 - val_loss: 0.0121\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0110 - val_loss: 0.0120\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0110 - val_loss: 0.0119\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0109 - val_loss: 0.0120\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0110 - val_loss: 0.0119\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0120\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0108 - val_loss: 0.0120\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0107 - val_loss: 0.0118\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0105 - val_loss: 0.0118\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0121\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0107 - val_loss: 0.0116\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0118\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0117\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0107 - val_loss: 0.0116\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0118\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0104 - val_loss: 0.0117\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0103 - val_loss: 0.0118\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0106 - val_loss: 0.0115\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0103 - val_loss: 0.0119\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0104 - val_loss: 0.0117\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0117\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0116\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0116\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0101 - val_loss: 0.0115\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0101 - val_loss: 0.0115\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0103 - val_loss: 0.0118\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0101 - val_loss: 0.0116\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0101 - val_loss: 0.0115\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 2s 35us/step - loss: 0.0102 - val_loss: 0.0116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpPOeGOWYct0",
        "colab_type": "text"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6jIeLFAVQR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfY4s--rYi3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "1e140fd5-599d-4e15-9487-3a80c9e710c6"
      },
      "source": [
        "plt.imshow(pred[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1ae028cc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv0il/g8iPPFcitHNtQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmlcQ_NaYlfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "15b9af0d-a475-4f51-ba8c-5daa36f9f37b"
      },
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1ae021d898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYetE8IY7b9",
        "colab_type": "text"
      },
      "source": [
        "This model performs almost similar to the vanilla autoencoder like in the image_reconstruction notebook. But on inspecting the output of the encoder model, we can see that it uses much more sparse information to represent the original image."
      ]
    }
  ]
}